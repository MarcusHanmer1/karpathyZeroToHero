{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd66c5d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#LOAD THE DATA OF COURSE\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "452750eb-089f-461d-95f8-f22e3d5a5cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#CHECKS\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "809655ac-5ffe-44c0-8872-02a2b053871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create an encoding from all of the chars we see in the data along with start and end symbols, and store the bigram occurances \n",
    "of these particular pairs in a tensor. NON neural network implementation\n",
    "\"\"\"\n",
    "import torch\n",
    "N = torch.zeros((27,27), dtype=torch.int32)\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "#Iterate through each word and count up the bigram information for each name\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "28076753-8af4-4a2b-b996-50bd5cd73294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 4410, 1306, 1542, 1690, 1531,  417,  669,  874,  591, 2422, 2963,\n",
       "        1572, 2538, 1146,  394,  515,   92, 1639, 2055, 1308,   78,  376,  307,\n",
       "         134,  535,  929], dtype=torch.int32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "49934b4f-33f1-4f3c-861f-26d0d8b2d5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a3e0627-7979-4ca7-916b-2ac2d879de21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maneladhar.\n",
      "ah.\n",
      "milir.\n",
      "titiashanadcyabe.\n",
      "zisan.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Bigram loop for word creating, normalise the vectors for a valid distribution and then just sample values until end character reached.\n",
    "\"\"\"\n",
    "P = N.float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "for i in range(5):\n",
    "    out = []\n",
    "    index = 0\n",
    "    while True:\n",
    "        p = P[index]\n",
    "        index = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6560c9bb-4f04-4bf4-a9fe-d618b4ca2dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNow we must ohe our particular bigram 'features' (first character) and 'targets' (second character), and do that within the gradient descent loop\\n\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Neural network implementation\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "num = xs.nelement()\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "W = torch.randn((27,27), requires_grad=True)\n",
    "\"\"\"\n",
    "Now we must ohe our particular bigram 'features' (first character) and 'targets' (second character), and do that within the gradient descent loop\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f2c93d9-ed2b-4c60-ac4d-1a6108a65688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.720113515853882\n",
      "2.7112045288085938\n",
      "2.7030258178710938\n",
      "2.6954712867736816\n",
      "2.6884593963623047\n",
      "2.6819231510162354\n",
      "2.6758086681365967\n",
      "2.670070171356201\n",
      "2.664670944213867\n",
      "2.659579277038574\n",
      "2.6547675132751465\n",
      "2.6502106189727783\n",
      "2.6458890438079834\n",
      "2.6417832374572754\n",
      "2.637876510620117\n",
      "2.6341545581817627\n",
      "2.630603075027466\n",
      "2.627211332321167\n",
      "2.623966932296753\n",
      "2.6208608150482178\n",
      "2.6178832054138184\n",
      "2.6150269508361816\n",
      "2.612283229827881\n",
      "2.6096460819244385\n",
      "2.6071085929870605\n",
      "2.6046650409698486\n",
      "2.6023101806640625\n",
      "2.60003924369812\n",
      "2.5978472232818604\n",
      "2.5957298278808594\n",
      "2.5936837196350098\n",
      "2.5917043685913086\n",
      "2.589789390563965\n",
      "2.587935209274292\n",
      "2.5861382484436035\n",
      "2.5843966007232666\n",
      "2.582707166671753\n",
      "2.5810675621032715\n",
      "2.5794761180877686\n",
      "2.577929973602295\n",
      "2.576427698135376\n",
      "2.5749664306640625\n",
      "2.5735456943511963\n",
      "2.5721631050109863\n",
      "2.570816993713379\n",
      "2.5695064067840576\n",
      "2.5682294368743896\n",
      "2.5669851303100586\n",
      "2.56577205657959\n",
      "2.5645885467529297\n",
      "2.563434362411499\n",
      "2.5623083114624023\n",
      "2.5612082481384277\n",
      "2.5601346492767334\n",
      "2.5590858459472656\n",
      "2.558060884475708\n",
      "2.5570592880249023\n",
      "2.5560801029205322\n",
      "2.5551226139068604\n",
      "2.5541863441467285\n",
      "2.553270101547241\n",
      "2.552372932434082\n",
      "2.551494598388672\n",
      "2.550635576248169\n",
      "2.5497937202453613\n",
      "2.548969030380249\n",
      "2.548161029815674\n",
      "2.5473697185516357\n",
      "2.54659366607666\n",
      "2.545832872390747\n",
      "2.5450870990753174\n",
      "2.5443551540374756\n",
      "2.54363751411438\n",
      "2.542933702468872\n",
      "2.5422425270080566\n",
      "2.5415642261505127\n",
      "2.540898323059082\n",
      "2.5402448177337646\n",
      "2.5396029949188232\n",
      "2.5389723777770996\n",
      "2.5383529663085938\n",
      "2.5377449989318848\n",
      "2.537147045135498\n",
      "2.536559581756592\n",
      "2.5359818935394287\n",
      "2.535413980484009\n",
      "2.534856081008911\n",
      "2.5343072414398193\n",
      "2.5337672233581543\n",
      "2.533236265182495\n",
      "2.5327138900756836\n",
      "2.5322000980377197\n",
      "2.5316946506500244\n",
      "2.5311970710754395\n",
      "2.5307071208953857\n",
      "2.5302250385284424\n",
      "2.5297505855560303\n",
      "2.5292835235595703\n",
      "2.5288236141204834\n",
      "2.5283701419830322\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    #forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "    print(loss.item())\n",
    "\n",
    "    #backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    W.data += -10 * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "64cea152-7a1c-48bd-aaa2-dae805f6221b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wukilalyll.\n",
      "letwee.\n",
      "i.\n",
      "bgpcty.\n",
      "thaml.\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    out = []\n",
    "    index = 0\n",
    "    while True:\n",
    "\n",
    "        xenc = F.one_hot(torch.tensor([index]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "        \n",
    "        index = torch.multinomial(p, num_samples=1, replacement=True).item()\n",
    "        out.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
